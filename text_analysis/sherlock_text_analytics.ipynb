{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elementary, My Dear Watson!\n",
    "# Text Analytics Tutorial using Sherlock Holmes Stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will demonstrate several text analytics techniques that can be used to analyze various text corpora in order to extract various interesting insights from the text. \n",
    "Namely, throughout this notebook, I will use [The Sherlock Holmes Books Collection](https://sherlock-holm.es/) to show how to (a) calculate various textual statistics; (b) create the social network among entities that appear in the books; (c) use a topic model to discover abstract topics in the text; and (d) utilize Word2Vec to find connections among various section of the text, and do some predictions.\n",
    "\n",
    "To perform the text analytics presented in this notebook, we will use [NLTK](http://www.nltk.org/) Python package, [GraphLab Create's](https://turi.com/products/create/) [SFrame](https://turi.com/products/create/docs/generated/graphlab.SFrame.html) and [SGraph](https://turi.com/products/create/docs/generated/graphlab.SGraph.html) objects, as well as GraphLab's [text analytics toolkit](https://turi.com/products/create/docs/graphlab.toolkits.text_analytics.html), and\n",
    "[Word2Vec](https://code.google.com/p/word2vec/) deep learning inspired model implemented in the [Gensim](http://radimrehurek.com/gensim/models/word2vec.html) Python package.\n",
    "\n",
    "\n",
    "The notebook is divided to the following sections:\n",
    "- <a href=\"#setup\">Setup</a>\n",
    "- <a href=\"#prepare\">Preparing and validating the dataset</a>\n",
    "- <a href=\"#stat\">Calculating Various Statistics</a>\n",
    "- <a href=\"#sn\">Constructing Social Networks</a>\n",
    "- <a href=\"#topic\">Topic Models</a>\n",
    "\n",
    "Each section can be executed independently. So feel free to skip ahead, just remember  to import all the required packages, and define all the needed functions.\n",
    "\n",
    "Required Python Packages:\n",
    "- [GraphLab Create](https://turi.com/products/create/quick-start-guide.html) - for classification, data.\n",
    "- [NLTK](http://www.nltk.org/) (including downloading [punkt](http://www.nltk.org/data.html)).\n",
    "- [Stanford Named Entity Recognizer](http://nlp.stanford.edu/software/CRF-NER.shtml) - for named entity recognition. \n",
    "- [Gensim](https://radimrehurek.com/gensim/) - for Word2Vec deep learning.\n",
    " engineering, and evaluation.\n",
    "- [pyLDAvis](https://github.com/bmabey/pyLDAvis) - for topic model visualziation. \n",
    "\n",
    "Let's do some text analytics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"setup\"></a>0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, make sure you have installed all the required Python packages. (The instructions below use pip. You can use easy_install, too.) Also, consider using [virtualenv](https://virtualenv.pypa.io/en/latest/) for a cleaner installation experience instead of sudo. I also recommend to run the code via [IPython Notebook](http://ipython.org/notebook.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "$ sudo pip install --upgrade gensim\n",
    "$ sudo pip install --upgrade nltk\n",
    "$ sudo pip install --upgrade graphlab-create\n",
    "$ sudo pip install --upgrade pyldavis\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need [a product key for GraphLab Create](https://turi.com/products/create/quick-start-guide.html), and to make [Stanford Named Entity Recognizer work with Pyhton NLTK](http://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages).\n",
    "\n",
    "After installing NLTK and from an interactive shell download the punkt model by importing nltk and running nltk.download(). From the resulting interactive window navigate to the model tab and select punkt and download to your system. \n",
    "\n",
    "To prepare the Stanford Named Entity Recognizer to work in your system make sure you read the following links : \n",
    "[1 - How to] (http://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages)\n",
    "[2 - API] (http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford)\n",
    "[3 - Stanford Parser FAQ] (http://nlp.stanford.edu/software/parser-faq.shtml)\n",
    "[4 - download] (http://nlp.stanford.edu/software/CRF-NER.html)\n",
    "5 - In the extracted directory it seems to help to run the stanford-ner.jar file in some computer configurations. \n",
    "\n",
    "Take note of what directory you downloaded and extracted the files to as you will need the location to pass the classifier and Stanford NER as arguments later in this notebook.\n",
    "\n",
    "Also in case you haven’t already, make sure you are running the latest [Java JDK] (http://www.oracle.com/technetwork/java/javase/downloads/index.html)\n",
    "\n",
    "For a quick test you can open an ipython shell and try the following:\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "st = StanfordNERTagger(‘[path_to_your_downloaded_package_classifiers_directory]/english.all.3class.distsim.crf.ser.gz', '[path_to_your_downloaded_package_root_directory]/stanford-ner.jar')\n",
    " st.tag('When we turned him over, the Boots recognized him at once as being the same gentleman who had engaged the room under the name of Joseph Stangerson.'.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"prepare\"></a>1. Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Constructing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<i>\"Data! Data! Data!\" he cried impatiently. \"I can't make bricks without clay.\"</i>\n",
    "                                                -The Adventure of the Copper Beeches\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook, we will be analyzing Sherlock Holmes's stories collection. \n",
    "So first, we will download the stories in ASCII format from the [sherlock-holm.es website](https://sherlock-holm.es/). \n",
    "sherlock-holm.es website contains over sixty downloadable stories, we will use the following code to download the stories and insert them into a SFrame object.\n",
    "\n",
    "<b> <u>Important Note:</u></b> in some countries, such as the U.S., few of Sherlock Holmes's books & stories are still under copyright restrictions. For more information, please advise the following [website](http://www.sherlockian.net/acd/copyright.html), and read the guidelines that appear in the end of [sherlock-holm.es ASCII](https://sherlock-holm.es/ascii/) download page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'link': '/stories/plain-text/cano.txt', 'title': 'The Complete Canon'}, {'link': '/stories/plain-text/cnus.txt', 'title': 'The Canon \\xe2\\x80\\x94 U.S. edition'}, {'link': '/stories/plain-text/advs.txt', 'title': 'The Adventures of Sherlock Holmes'}, {'link': '/stories/plain-text/mems.txt', 'title': 'The Memoirs of Sherlock Holmes'}, {'link': '/stories/plain-text/retn.txt', 'title': 'The Return of Sherlock Holmes'}, {'link': '/stories/plain-text/lstb.txt', 'title': 'His Last Bow'}, {'link': '/stories/plain-text/case.txt', 'title': 'The Case-Book of Sherlock Holmes'}, {'link': '/stories/plain-text/stud.txt', 'title': 'A Study In Scarlet'}, {'link': '/stories/plain-text/sign.txt', 'title': 'The Sign of the Four'}, {'link': '/stories/plain-text/houn.txt', 'title': 'The Hound of the Baskervilles'}, {'link': '/stories/plain-text/vall.txt', 'title': 'The Valley of Fear'}, {'link': '/stories/plain-text/scan.txt', 'title': 'A Scandal in Bohemia'}, {'link': '/stories/plain-text/redh.txt', 'title': 'The Red-Headed League'}, {'link': '/stories/plain-text/iden.txt', 'title': 'A Case of Identity'}, {'link': '/stories/plain-text/bosc.txt', 'title': 'The Boscombe Valley Mystery'}, {'link': '/stories/plain-text/five.txt', 'title': 'The Five Orange Pips'}, {'link': '/stories/plain-text/twis.txt', 'title': 'The Man with the Twisted Lip'}, {'link': '/stories/plain-text/blue.txt', 'title': 'The Adventure of the Blue Carbuncle'}, {'link': '/stories/plain-text/spec.txt', 'title': 'The Adventure of the Speckled Band'}, {'link': '/stories/plain-text/engr.txt', 'title': \"The Adventure of the Engineer's Thumb\"}, {'link': '/stories/plain-text/nobl.txt', 'title': 'The Adventure of the Noble Bachelor'}, {'link': '/stories/plain-text/bery.txt', 'title': 'The Adventure of the Beryl Coronet'}, {'link': '/stories/plain-text/copp.txt', 'title': 'The Adventure of the Copper Beeches'}, {'link': '/stories/plain-text/silv.txt', 'title': 'Silver Blaze'}, {'link': '/stories/plain-text/yell.txt', 'title': 'Yellow Face'}, {'link': '/stories/plain-text/stoc.txt', 'title': \"The Stockbroker's Clerk\"}, {'link': '/stories/plain-text/glor.txt', 'title': 'The \\xe2\\x80\\x9cGloria Scott\\xe2\\x80\\x9d'}, {'link': '/stories/plain-text/musg.txt', 'title': 'The Musgrave Ritual'}, {'link': '/stories/plain-text/reig.txt', 'title': 'The Reigate Puzzle'}, {'link': '/stories/plain-text/croo.txt', 'title': 'The Crooked Man'}, {'link': '/stories/plain-text/resi.txt', 'title': 'The Resident Patient'}, {'link': '/stories/plain-text/gree.txt', 'title': 'The Greek Interpreter'}, {'link': '/stories/plain-text/nava.txt', 'title': 'The Naval Treaty'}, {'link': '/stories/plain-text/fina.txt', 'title': 'The Final Problem'}, {'link': '/stories/plain-text/empt.txt', 'title': 'The Empty House'}, {'link': '/stories/plain-text/norw.txt', 'title': 'The Norwood Builder'}, {'link': '/stories/plain-text/danc.txt', 'title': 'The Dancing Men'}, {'link': '/stories/plain-text/soli.txt', 'title': 'The Solitary Cyclist'}, {'link': '/stories/plain-text/prio.txt', 'title': 'The Priory School'}, {'link': '/stories/plain-text/blac.txt', 'title': 'Black Peter'}, {'link': '/stories/plain-text/chas.txt', 'title': 'Charles Augustus Milverton'}, {'link': '/stories/plain-text/sixn.txt', 'title': 'The Six Napoleons'}, {'link': '/stories/plain-text/3stu.txt', 'title': 'The Three Students'}, {'link': '/stories/plain-text/gold.txt', 'title': 'The Golden Pince-Nez'}, {'link': '/stories/plain-text/miss.txt', 'title': 'The Missing Three-Quarter'}, {'link': '/stories/plain-text/abbe.txt', 'title': 'The Abbey Grange'}, {'link': '/stories/plain-text/seco.txt', 'title': 'The Second Stain'}, {'link': '/stories/plain-text/wist.txt', 'title': 'Wisteria Lodge'}, {'link': '/stories/plain-text/card.txt', 'title': 'The Cardboard Box'}, {'link': '/stories/plain-text/redc.txt', 'title': 'The Red Circle'}, {'link': '/stories/plain-text/bruc.txt', 'title': 'The Bruce-Partington Plans'}, {'link': '/stories/plain-text/dyin.txt', 'title': 'The Dying Detective'}, {'link': '/stories/plain-text/lady.txt', 'title': 'Lady Frances Carfax'}, {'link': '/stories/plain-text/devi.txt', 'title': \"The Devil's Foot\"}, {'link': '/stories/plain-text/last.txt', 'title': 'His Last Bow'}, {'link': '/stories/plain-text/illu.txt', 'title': 'The Illustrious Client'}, {'link': '/stories/plain-text/blan.txt', 'title': 'The Blanched Soldier'}, {'link': '/stories/plain-text/maza.txt', 'title': 'The Mazarin Stone'}, {'link': '/stories/plain-text/3gab.txt', 'title': 'The Three Gables'}, {'link': '/stories/plain-text/suss.txt', 'title': 'The Sussex Vampire'}, {'link': '/stories/plain-text/3gar.txt', 'title': 'The Three Garridebs'}, {'link': '/stories/plain-text/thor.txt', 'title': 'Thor Bridge'}, {'link': '/stories/plain-text/cree.txt', 'title': 'The Creeping Man'}, {'link': '/stories/plain-text/lion.txt', 'title': \"The Lion's Mane\"}, {'link': '/stories/plain-text/veil.txt', 'title': 'The Veiled Lodger'}, {'link': '/stories/plain-text/shos.txt', 'title': 'Shoscombe Old Place'}, {'link': '/stories/plain-text/reti.txt', 'title': 'The Retired Colourman'}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib2\n",
    "import graphlab as gl\n",
    "\n",
    "# NOTE: Update BASE_DIR to your own directory path\n",
    "BASE_DIR = \"~/repos/statistics-indonesia-python/text_analysis/data\" \n",
    "\n",
    "books_url = \"http://sherlock-holm.es/ascii/\"\n",
    "re_books_links = re.compile(\"\\\"piwik_download\\\"\\s+href=\\\"(?P<link>.*?)\\\">(?P<title>.*?)</a>\", re.MULTILINE)\n",
    "html = urllib2.urlopen(books_url).read()\n",
    "books_list = [m.groupdict() for m in re_books_links.finditer(html)]\n",
    "print books_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the books' titles and links, now let's download the books' texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter books due to copyright issues. In this code, we filtered \"The Complete Canon\", “Case-Book of Sherlock Holmes” books, and\n",
    "# \"The Canon — U.S. edition\" book (For more information please read the note above).\n",
    "filtered_books = set([\"The Complete Canon\", \"The Case-Book of Sherlock Holmes\", \"The Canon — U.S. edition\" ])\n",
    "books_list = filter(lambda d: d['title'] not in filtered_books, books_list )\n",
    "\n",
    "#Download books' texts (to not overload the website we download the text in batch and not in parallel)\n",
    "for d in books_list:\n",
    "    d['text'] = urllib2.urlopen(\"http://sherlock-holm.es\" + d['link']).read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dict list into a SFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf = gl.SFrame(books_list).unpack(\"X1\", column_name_prefix=\"\")\n",
    "sf.save(\"%s/books.sframe\" % BASE_DIR)\n",
    "sf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"stat\"></a>2. Calculating Various Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will demonstrate  how it is very straight-forward to utilize GraphLab Create SFrame object to calculate & visualize various statistics.\n",
    "\n",
    "In previous section, we created a SFrame object which consists of 64 texts. Let us first load the SFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "import re\n",
    "\n",
    "# NOTE: Update BASE_DIR to your own directory path\n",
    "BASE_DIR = \"~/repos/statistics-indonesia-python/text_analysis/data\" \n",
    "gl.canvas.set_target('ipynb')\n",
    "sf = gl.load_sframe(\"%s\\\\books.sframe\" % BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python, it is very easy to calculate the number of characters  in a text, we just need to use the built-in [len function](https://docs.python.org/2/library/functions.html#len). Let's calculate the number of characters  in each downloaded text using the the <i>len</i> function and SArray's [<i>apply</i> function](https://turi.com/products/create/docs/generated/graphlab.SArray.apply.html#graphlab.SArray.apply) (notice that each column in a [SFrame object](https://turi.com/products/create/docs/generated/graphlab.SFrame.html) is a [SArray object](https://turi.com/products/create/docs/generated/graphlab.SArray.html?highlight=sarray))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf['chars_num'] = sf['text'].apply(lambda t: len(t))\n",
    "sf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the [show function](https://turi.com/products/create/docs/generated/graphlab.SArray.show.html) to visualize  the distribution  of text length in each one of our downloaded text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf['chars_num'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean characters  number in the download stories is 95020.42, and the maximal number of characters  in a story is 662,242 characters.\n",
    "Let's also calculate  the number of words in each text. Calculating the number of words in a text is little trickier and there are several methods to perform this task. Using one of the following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"\"\"I think that you know me well enough, Watson, to understand that I am by no means a nervous man. At the same time,\n",
    "it is stupidity rather than courage to refuse to recognize danger when it is close upon you.\"\"\"\n",
    "\n",
    "#using the split function\n",
    "print text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using NLTK \n",
    "#Note: Remember to download the NLTK's punkt package by running nltk.download() from the Interactive Python Shell\n",
    "import nltk\n",
    "print nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that both using the split function, or using the regular expression work pretty-well. \n",
    "However, it is important to notice, that the first regular expression can mistakenly split words, \n",
    "such as \"S.H.\" into two words, while the split function doesn't remove punctuation. \n",
    "Therefore, if we want to be precise, we can use the NLTK's tokenize package and remove punctuation from the results.\n",
    "Nevertheless, for our case, it is good enough to use the regular expression method to count words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_words_split = re.compile(\"(\\w+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf['words_num'] = sf['text'].apply(lambda t: len(re_words_split.findall(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use NLTK to count the number of sentences in each story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def txt2sentences(txt, remove_none_english_chars=True):\n",
    "    \"\"\"\n",
    "    Split the English text into sentences using NLTK\n",
    "    :param txt: input text.    \n",
    "    :param remove_none_english_chars: if True then remove none English chars from text\n",
    "    :return: string in which each line consists of single sentence from the original input text.\n",
    "    :rtype: str\n",
    "    \"\"\"        \n",
    "    # decode to utf8 to avoid encoding problems - if someone has better idea how to solve encoding \n",
    "    # problem I will love to learn about it.     \n",
    "    txt = txt.decode(\"utf8\") \n",
    "    # split text into sentences using NLTK package\n",
    "    for s in tokenizer.tokenize(txt):\n",
    "        if remove_none_english_chars:\n",
    "            #remove none English chars\n",
    "            s = re.sub(\"[^a-zA-Z]\", \" \", s)\n",
    "        yield s\n",
    "\n",
    "sf['sentences_num'] = sf['text'].apply(lambda t: len(list(txt2sentences(t))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf[['chars_num','words_num','sentences_num']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now I calculated very basic text statistics. Let's try to do something more complicated like count the number of time the words 'Sherlock',\n",
    "'Watson', and 'Elementary' appeared in each story. We will do it using GraphLab's [text_analytics.count_words](https://turi.com/products/create/docs/generated/graphlab.text_analytics.count_words.html#graphlab.text_analytics.count_words) toolkit.\n",
    "\n",
    "<u>Note</u>: To count the frequency a word appears in a text, one can also consider using the [collection.Counter](https://docs.python.org/2/library/collections.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf['words_count'] = gl.text_analytics.count_words(sf['text'], to_lower=True)\n",
    "sf['sherlock_count'] = sf['words_count'].apply(lambda d: d.get('sherlock',0))\n",
    "sf['watson_count'] = sf['words_count'].apply(lambda d: d.get('watson',0))\n",
    "sf['elementary_count'] = sf['words_count'].apply(lambda d: d.get('elementary',0))\n",
    "sf[['sherlock_count', 'watson_count', 'elementary_count']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is nice to see that the mean number of times the word 'Sherlock' appear in the stories is 9.609 times while the mean the word 'Watson' appear is only 1.734.\n",
    "Moreover, there are stories, such as [The Adventure of the Lion's Mane](https://sherlock-holm.es/stories/pdf/a4/1-sided/lion.pdf) that the word 'Sherlock' doesn't appear even once. \n",
    "\n",
    "Let's try to use simple linear regression to predict the number of times the word 'Sherlock' appear in a text based on the number of time the word 'Watson' appear in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_reg = gl.linear_regression.create(sf, target='sherlock_count', features=['watson_count'])\n",
    "linear_reg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the simple linear regression, we have the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$sherlock_{count} = 2.1404*watson_{count} + 5.8972$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of other really interesting insights that one can discover using similar methodology. I leave the reader to discover these insights by themselves. Let's move to the next section\n",
    "and create some nice graphs using various entity extraction tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sn\"></a>3. Constructing Social Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<i>\"Listen, what I said before John, I meant it. I don’t have friends; I’ve just got one.\"\n",
    "                                               -Sherlock, The Hounds of Baskerville, 2012 </i>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of my main fields of interest are social networks. I love to study and visualize graphs of [various types of networks](http://proj.ise.bgu.ac.il/sns/gallery.html). One of the nice [studies](http://www.technologyreview.com/view/516081/the-remarkable-properties-of-mythological-social-networks/) that I read not long ago showes that it is possible to create the social network of book characters. For example, [Miranda et al.](http://arxiv.org/abs/1306.2537) built and analyzed a social network utilizing the Odyssey of Homer. \n",
    "\n",
    "To manually create a precise social network between Sherlock Holmes characters, we can read the stories and whenever two characters have a conversation, or appear in the same scene, we add to the network nodes with the two characters names (if there are not in the network already), and create a link between the two characters. In case, we want to create a weighted social network, we can also add a weight to each link with the number of times each two characters talked to each other. \n",
    "\n",
    "When processing a large text corpus, manually using this process to construct a social network can very time-consuming. Therefore, we would like to perform this process automatically. One of the ways to consturct the social network is by using various NLP algorithms that analyze the text and \"understand\" the relationships between two entities. However, I am not familiar with open source tools that can analyze a text corpus and infer the connections between two entities with high precision. \n",
    "\n",
    "In this section, I will demonstrate some very simple techniques that can be utilized to study the social connections among characters in Sherlock Holmes stories. These techniques won't create the most precise social network. However, the created network is sufficient to observe some interesting insights about the relationships among the stories' characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Constructing Social Network using Names List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this techniques, we will split the downloaded Sherlock Holmes stories into sentences, and using a predefined list of names of book characters we will create a social network with links among the stories characters by adding a link between each two characters that appear in the same sentence. Let start constructing the social network by splitting the stories into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "import re,nltk\n",
    "gl.canvas.set_target('ipynb')\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def txt2sentences(txt, remove_none_english_chars=True):\n",
    "    \"\"\"\n",
    "    Split the English text into sentences using NLTK\n",
    "    :param txt: input text.    \n",
    "    :param remove_none_english_chars: if True then remove none English chars from text\n",
    "    :return: string in which each line consists of single sentence from the original input text.\n",
    "    :rtype: str\n",
    "    \"\"\"        \n",
    "    txt = txt.decode(\"utf8\") \n",
    "    # split text into sentences using nltk packages\n",
    "    for s in tokenizer.tokenize(txt):\n",
    "        if remove_none_english_chars:\n",
    "            #remove none English chars\n",
    "            s = re.sub(\"[^a-zA-Z]\", \" \", s)\n",
    "        yield s\n",
    "        \n",
    "sf = gl.load_sframe(\"%s/books.sframe\" % BASE_DIR)\n",
    "sf['sentences'] = sf['text'].apply(lambda t: list(txt2sentences(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sf_sentences = sf.flat_map(['title', 'text'], lambda t: [[t['title'],s.strip()] for s in txt2sentences(t['text'])])\n",
    "sf_sentences = sf_sentences.rename({'text': 'sentence'})\n",
    "\n",
    "#split each sentence into words\n",
    "sf_sentences['words'] = sf_sentences['sentence'].apply(lambda s:re_words_split.findall(s))\n",
    "sf_sentences.save(\"%s/sentences.sframe\" % BASE_DIR)\n",
    "sf_sentences.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We created a SFrame named <i>sf_sentences</i> in which each row contains a single sentence. Now let's find out which two or more characters from the following [link](http://www.wikiwand.com/en/Category:Sherlock_Holmes_characters) appear in the same sentences. Notice that we only use the characters unique names so we don't mix up between characters with similar names. For example, the name Holmes can represent both Sherlock Holmes and Mycroft Holmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_characters_set = set([\"Irene\",\"Mycroft\",\"Lestrade\",\"Sherlock\",\"Moran\",\"Moriarty\",\"Watson\" ])\n",
    "sf_sentences['characters'] = sf_sentences['words'].apply(lambda w: list(set(w) & main_characters_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the <i>'characters'</i> column contain the names of the main characters that appear in the same sentences together. Let's use this information to create the characters social network by constructing a SGraph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "from graphlab import SGraph, Vertex, Edge\n",
    "\n",
    "def get_characters_graph(sf, min_edge_strength=1):\n",
    "    \"\"\"\n",
    "    Constructs a social network from the an input SFrame. In the social network the verticies are the characters\n",
    "    and the edges are only between characters that appear in the same sentence at least min_edge_strength times\n",
    "    :param sf: input SFrame object that contains 'characters' column   \n",
    "    :param min_edge_strength: minimal connetion strength between two characters.  \n",
    "    :return: SGraph object constructed from the input SFrame. The graph only contains edges with \n",
    "        the at least the input minimal strength between between the characters.\n",
    "    :rtype: gl.SGraph\n",
    "    \"\"\"\n",
    "    #filter sentences with less than two characters\n",
    "    sf['characters_num'] = sf['characters'].apply(lambda l: len(l))\n",
    "    sf = sf_sentences[sf['characters_num'] > 1]\n",
    "    characters_links = []\n",
    "    for l in sf['characters']:    \n",
    "        # if there are more than two characters in the same sentences. Create all link combinations between\n",
    "        # all the characters (order doesn't matter)\n",
    "        characters_links += itertools.combinations(l,2)\n",
    "\n",
    "    #calculating the connections strength between each two characters\n",
    "    c = Counter(characters_links)\n",
    "    g = SGraph()\n",
    "\n",
    "    edges_list = []\n",
    "    for l,s in c.iteritems():    \n",
    "        if s < min_edge_strength:\n",
    "            # filter out connections that appear less than min_edge_strength\n",
    "            continue\n",
    "        edges_list.append(Edge(l[0], l[1], attr={'strength':s}))\n",
    "\n",
    "    g = g.add_edges(edges_list)\n",
    "    return g\n",
    "\n",
    "g = get_characters_graph(sf_sentences)\n",
    "g.show(vlabel=\"__id\", elabel=\"strength\", node_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Sherlock's social network, it can be noticed that Sherlock has two main social circles. The first one is circle of friends that include Mycroft and Lestrade. Additionally, he has a circle of enemies that include Moriaty and Moran. Additionally, we can notice Watson is strongly connected to Sherlock and Sherlock's nemesis Moriaty. <br>Let's repeat the experiments only this time we also add minor characters from the following [link](http://www.wikiwand.com/en/Minor_Sherlock_Holmes_characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minor_characters_set = set([\"Irene\",\"Mycroft\",\"Lestrade\",\"Sherlock\",\"Moran\",\"Moriarty\",\"Watson\",\"Baynes\",\"Billy\",\"Bradstreet\",\"Gregson\"\n",
    "                            ,\"Hopkins\",\"Hudson\",\"Shinwell\",\"Athelney\",\"Mary\",\"Langdale\",\"Toby\",\"Wiggins\"])\n",
    "\n",
    "sf_sentences['characters'] = sf_sentences['words'].apply(lambda w: list(set(w) & minor_characters_set))\n",
    "sf_sentences['characters_num'] = sf_sentences['characters'].apply(lambda l: len(l))\n",
    "sf_sentences = sf_sentences[sf_sentences['characters_num'] > 1]\n",
    "\n",
    "g = get_characters_graph(sf_sentences)\n",
    "g.show(vlabel=\"__id\", elabel=\"strength\", node_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a more complex social network with the additional minor characters.\n",
    "I believe this social network graph can be improved by increasing the scope of characters search from single sentence to multiple sentences, or by using characters additional names and nick names. I leave the reader to try to improve the graph by themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Constructing Social Network using Named Entity Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the disadvantages of the above method is that you need a predefined list of names to create the social network. However, in many cases this list is unavailable. Therefore, we need another method to find entities in the text. One common method to achieve this is using [Named Entity Recognition](http://www.wikiwand.com/en/Named-entity_recognition) (NER). By using NER algorithms, we can classify elements in the text into pre-defined categories, such as the names of persons, organizations, and locations. There are many tools that can perform NER, such as [OpenNLP](http://www.wikiwand.com/en/OpenNLP), [Stanford Named Entity Recognizer]( http://nlp.stanford.edu/software/CRF-NER.shtml), [Rosette Entity Extractor](http://www.basistech.com/text-analytics/rosette/entity-extractor/). In this notebook, we will use the Stanford Named Entity Recognizer via NLTK. We will use NER algorithms to automatically  construct an entity list of the most common characters of the book.\n",
    "\n",
    "Please note that making NLTK run Stanford Named Entity Recognizer can be non-trivial. For more details, on how to make NLTK work with Stanford Named Entity Recognizer please read the information provided in the following links [1](http://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages),[2](http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford), & [3](http://nlp.stanford.edu/software/parser-faq.shtml#). \n",
    "\n",
    "NOTE: running the next code section can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "sf_books =  gl.load_sframe(\"%s/books.sframe\" % BASE_DIR)\n",
    "\n",
    "#IMPORTANT: The directory that include the Stanford Named Entity Recognizer files it need to be updated according \n",
    "# to the local installation directory\n",
    "STANFORD_DIR = BASE_DIR + \"/stanford-ner-2015-12-09/\"\n",
    "\n",
    "#need to insert as parameters the stanford-ner.jar and the type of classifier we want to use\n",
    "\n",
    "st = StanfordNERTagger('/Users/kivan/Documents/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                       '/Users/kivan/Documents/stanford-ner-2015-12-09/stanford-ner.jar')\n",
    "\n",
    "st.java_options = \"-Xmx4096m\"\n",
    "\n",
    "sf_books['sentences'] = sf_books['text'].apply(lambda t: list(txt2sentences(t)))\n",
    "sf_books['words'] = sf_books['sentences'].apply(lambda l: [re_words_split.findall(s) for s in l])\n",
    "sf_books['NER'] = sf_books['words'].apply(lambda w: st.tag_sents(w))\n",
    "sf_books['person'] = sf_books['NER'].apply(lambda n: [e[0] for s in n for e in s if e[1] == 'PERSON'])\n",
    "\n",
    "person_list = []\n",
    "for p in sf_books['person']:\n",
    "    person_list += p\n",
    "    \n",
    "print len(set(person_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(person_list)\n",
    "# We are removing some mistken classified words, too common names, and etc. to make the constructed social network\n",
    "# more readable.\n",
    "characters_set = set(i[0] for i in c.most_common(200)) - set(['the', 'You', 'Mrs', 'He', 'Dr', 'me','did', 'Mr', \n",
    "                                      'Now', 'My', 'Miss', 'of', 'Sir', 'Here', 'All', 'Our', 'sir',\n",
    "                                      'man', 'father', 'What', 'There', 'When', 'no', 'Lord', 'you', 'St',\n",
    "                                      'John', 'James',  'Holmes', 'Arthur', 'Conan', 'Doyle', 'Lady'])\n",
    "\n",
    "sf_sentences = gl.load_sframe(\"%s/sentences.sframe\" % BASE_DIR)\n",
    "sf_sentences['characters'] = sf_sentences['words'].apply(lambda w: list(set(w) & characters_set))\n",
    "sf_sentences['characters_num'] = sf_sentences['characters'].apply(lambda l: len(l))\n",
    "sf_sentences = sf_sentences[sf_sentences['characters_num'] > 1]\n",
    "g = get_characters_graph(sf_sentences, min_edge_strength=3)\n",
    "print g.summary()\n",
    "g.show(vlabel=\"__id\", elabel=\"strength\", node_size=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adding a function to clean the graph as in some cases the Stanford NER maps 'I' as a person.\n",
    "def clean_graph(g, remove_entities_set):\n",
    "    vertices = g.vertices[g.vertices[\"__id\"].apply(lambda v: v not in remove_entities_set)] \n",
    "    edges = g.edges[g.edges.apply(lambda e: e[\"__src_id\"] not in remove_entities_set and e[\"__dst_id\"] not in remove_entities_set)]\n",
    "    return gl.SGraph(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cleaning the graph and displaying it again\n",
    "g = clean_graph(g, {\"I\"})\n",
    "g.show(vlabel=\"__id\", elabel=\"strength\", node_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NER algorithm did pretty good job, and most of the names of the identified entities looks logical (at least to me).\n",
    "Additionally, we can understand the link between the various book characters. We can also notice that in many of the graph's components that have only two vertices  the connection is between each characfter first and it's last names. Let use GraphLab [graph_analytics toolkit](https://turi.com/products/create/docs/graphlab.toolkits.graph_analytics.html) and focus on the social network's largest component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_graph_largest_compnent(g):\n",
    "    \"\"\"\n",
    "    Returns a graph with the largest component of the input graph\n",
    "    :param g: input graph (SGraph object)\n",
    "    :return: a graph of the largest component in the input object\n",
    "    :rtype: gl.SGraph\n",
    "    \"\"\"\n",
    "    \n",
    "    cc = gl.connected_components.create(g)    \n",
    "    #add each vertices its component id\n",
    "    g.vertices['component_id'] = cc['graph'].vertices['component_id']\n",
    "    # calculate the component id of the largest component\n",
    "    largest_component_id = cc['component_size'].sort('Count', ascending=False)[0]['component_id']\n",
    "    largest_component_verticies = g.vertices.filter_by(largest_component_id, 'component_id')['__id']\n",
    "    h = g.get_neighborhood(largest_component_verticies, 1)\n",
    "    return h\n",
    "\n",
    "h = get_graph_largest_compnent(g)  \n",
    "h.show(vlabel=\"__id\", elabel=\"strength\", node_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = clean_graph(g, {\"I\"})\n",
    "h.show(vlabel=\"__id\", elabel=\"strength\", node_size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "According the above graph that can be created almost automatically. We can easily identify the main characters in the stories. Additionally, we can observe that the strongest connection is between Sherlock and Watson.\n",
    "Moreover, we can see various connections among the main and minor characters of the book. However, from only looking at the graph, it is non-trivial to understand the various communities and their relationships.\n",
    "\n",
    "Using similar methods, we can learn more on each character by finding connections among person and location and person and organization. I leave the reader to find additional insights on the various characters on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## <a id=\"topic\"></a> 4. Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<i>\"I have known him for some time,\" said I, \"but I never knew him do anything yet without a very good reason, and with that our conversation drifted off on to other topics.</i>\n",
    "                                                                 -Memoirs of Sherlock Holmes\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Wikipedia [article](https://en.wikipedia.org/wiki/Topic_model), topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. I personally find topic models an interesting tool to explore a large text corpus. In this section, we are going to demonstrate how it is possible to utilze GraphLab's [topic model toolkit](https://turi.com/products/create/docs/graphlab.toolkits.topic_model.html) with the [pyLDAvis](https://github.com/bmabey/pyLDAvis) package to uncover topics in a set of documents. Namely, we will use GraphLab's topic model toolkit to analyze paragraphs in Sherlock Holmes stories. \n",
    "\n",
    "We will start by separating each story into paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "import re\n",
    "\n",
    "sf =  gl.load_sframe(\"%s/books.sframe\" % BASE_DIR)\n",
    "sf_paragraphs = sf.flat_map(['title', 'text'], lambda t: [[t['title'],p.strip()] for p in t['text'].split(\"\\n\\n\")])\n",
    "sf_paragraphs = sf_paragraphs.rename({'text': 'paragraph'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the number of words in each paragraph, and filter the paragraph that have less than 25 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re_words_split = re.compile(\"(\\w+)\")\n",
    "sf_paragraphs['paragraph_words_number'] = sf_paragraphs['paragraph'].apply(lambda p: len(re_words_split.findall(p)) )\n",
    "sf_paragraphs = sf_paragraphs[sf_paragraphs['paragraph_words_number'] >=25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the stories' paragraphs as documents, we can utilize GraphLab's topic model toolkit to discover topics that appear in these paragraph.\n",
    "We create a topic model with 10 topics to learn.\n",
    "\n",
    "Note: the topic model results may be different in each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs =  gl.text_analytics.count_ngrams(sf_paragraphs['paragraph'], n=1)\n",
    "stopwords = gl.text_analytics.stopwords()\n",
    "# adding some additional stopwords to make the topic model more clear\n",
    "stopwords |= set(['man', 'mr', 'sir', 'make', 'made', 'll', 'door', 'long', 'day', 'small']) \n",
    "docs = docs.dict_trim_by_keys(stopwords, exclude=True)\t\n",
    "docs = docs.dropna()\n",
    "topic_model = gl.topic_model.create(docs, num_topics=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the most common word in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_model.get_topics().print_rows(100)\n",
    "topic_model.save(\"%s/topic_model\" % BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the above table, we can understand some of the topics. However, it still hard to get good overall overview.\n",
    "Therefore, we will use the excellent [pyLDAvis package](https://github.com/bmabey/pyLDAvis/blob/master/README.rst), developed by Ben Mabey,\n",
    "to better the various topics in the books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.graphlab\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.graphlab.prepare(topic_model, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the above visualization, we can observe that the algorithm returned pretty interesting results. For example, one identified topic is related to Watson, locations (room, street, house, etc.), and time (days, hours, etc.). While, another topic is related to Holmes, men, and murder. For me these are pretty interesting results. I recommend the reader to try investigate the results by themselves. Moreover, I think that running the topic model algorithm on other text corpus can help to better understand this algorithms advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"word2vec\"></a> 5. Finding SImilar Paragraphs using Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\"I have notes of several similar cases, though none, as I remarked before, which were quite as prompt. My whole examination served to \n",
    "turn my conjecture into a certainty. Circumstantial evidence is occasionally very convincing, as when you find a trout in the milk, to \n",
    "quote Thoreau's example.\"\n",
    "                                                                                                 -The Adventure of the Noble Bachelor\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These days, no NLP related post can be complete without including the words \"deep learning.\" Therefore, in this section I will demonstrate how to use Word2Vec deep learning inspired algorithm to search for paragraphs that have similar text or writing style. \n",
    "\n",
    "First, let's build a Word2Vec model using Sherlock's stories. We will construct the Word2Vec model using the Gensim package and a similar method to the one presented in [Word2vec Tutorial](http://rare-technologies.com/word2vec-tutorial/) and in my previous [post](https://turi.com/learn/gallery/notebooks/deep_text_learning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "import urllib2\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "txt = urllib2.urlopen(\"https://sherlock-holm.es/stories/plain-text/cnus.txt\").read()\n",
    "\n",
    "re_words_split = re.compile(\"(\\w+)\")\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "def txt2words(s):\n",
    "    s = re.sub(\"[^a-zA-Z]\", \" \", s).lower()\n",
    "    return re_words_split.findall(s)\n",
    "\n",
    "class MySentences(object):\n",
    "        def __init__(self, txt):\n",
    "            self._txt = txt.decode(\"utf8\") \n",
    "            \n",
    "        def __iter__(self):\n",
    "            \"\"\"\n",
    "            Split the English text into sentences and then to words using NLTK\n",
    "            :param txt: input text.    \n",
    "            :param remove_none_english_chars: if True then remove none English chars from text\n",
    "            :return: list of words in which each list consists of single sentence's words from the original input text.\n",
    "            :rtype: str\n",
    "            \"\"\"                \n",
    "            # split text into sentences using NLTK package\n",
    "            for s in tokenizer.tokenize(self._txt):                                    \n",
    "                yield txt2words(s)\n",
    "\n",
    "sentences = MySentences(txt)\n",
    "model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=3, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a trained Word2Vec model, let's see if it gives reasonable results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.most_similar(\"watson\")\n",
    "print model.most_similar(\"holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got that the most similar word to Watson is Mortimer and the most similar word to Holmes is Lestrade. These results sound logical enough. Let us calculate the average vector of each paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# NOTE: Update BASE_DIR to your own directory path\n",
    "BASE_DIR = r\"~/repos/statistics-indonesia-python/text_analysis/data\" \n",
    "sf =  gl.load_sframe(\"%s/books.sframe\" % BASE_DIR)\n",
    "sf_paragraphs = sf.flat_map(['title', 'text'], lambda t: [[t['title'],p.strip()] for p in t['text'].split(\"\\n\\n\")])\n",
    "sf_paragraphs = sf_paragraphs.rename({'text': 'paragraph'})\n",
    "sf_paragraphs['paragraph_words_number'] = sf_paragraphs['paragraph'].apply(lambda p: len(re_words_split.findall(p)) )\n",
    "sf_paragraphs = sf_paragraphs[sf_paragraphs['paragraph_words_number'] >=25]\n",
    "\n",
    "def txt2avg_vector(txt, w2v_model):\n",
    "    words = [w for w in txt2words(txt.lower()) if w in w2v_model]\n",
    "    v = np.mean([w2v_model[w] for w in words],axis=0)    \n",
    "    return v\n",
    "\n",
    "sf_paragraphs['mean_vector'] = sf_paragraphs['paragraph'].apply(lambda p: txt2avg_vector(p, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the mean vector value of each paragraph. Let's utilize [GraphLab Create nearest neighbors toolkit](https://turi.com/products/create/docs/graphlab.toolkits.nearest_neighbors.html) to identify paragraphs that have similar text or  writing style. We will acheive that by calaculating the nearest neighbor to each the mean vector of each paragraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#construncting nearest neighbors model\n",
    "nn_model = gl.nearest_neighbors.create(sf_paragraphs, features=['mean_vector'])\n",
    "\n",
    "#calaculating the two nearest neighbors of each paragraph from all the paragraphs \n",
    "r = nn_model.query(sf_paragraphs, k=2)\n",
    "r.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course the nearest neighbors to each paragraph is the paragraph itself. Therefore, let us filter out paragraph that are with a distance of zero from each other. Additionally, let's look only on two near paragraphs that have small distance from each other (distance < 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filter out paragraphs that are exactly exactly the same\n",
    "r = r[r['distance'] != 0]\n",
    "\n",
    "#filter out paragraphs that are with distance >= 0.1\n",
    "r = r[r['distance'] < 0.08]\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use join to match between each query_label and reference_label values and their actual paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_paragraphs = sf_paragraphs.add_row_number('query_label')\n",
    "sf_paragraphs = sf_paragraphs.add_row_number('reference_label')\n",
    "sf_similar = r.join(sf_paragraphs, on=\"query_label\").join(sf_paragraphs, on=\"reference_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf_similar[['paragraph','title', 'title.1', 'paragraph.1', 'distance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the similar paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sf_similar[1]['paragraph']\n",
    "print \"-\"*100\n",
    "print sf_similar[1]['paragraph.1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although that in the paragraphs match the text is completely different, it still has the several similar motifs. In the first paragraph dog is leading his master. \n",
    "While in the second paragraph the boots are replacing the dog part. In both paragraphs the author use somewhat similar motifs \"dreadful sight to see that huge black creature\" \n",
    "and \"saw something that made me feel sickish.\" I personally find these results quite interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"go\"></a> 6. Where to Go From Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\"Thank you,\" said Holmes, \"I only wished to ask you how you would go from here to the Strand.\"\n",
    "                                                                              -The Red-Headed League\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we presented a short and practical tutorial for NLP, which covered several common NLP topics, such as NER, Topic Model, and Word2Vec. If you want to continue to explore this dataset yourself, there are a lot more that can be done. You can rerun this code using different texts (Harry Potter, Lord of the Rings, and etc.). In addition, you can try to modify the above code to create social networks between persons and locations, or to use [GloVe]( http://nlp.stanford.edu/projects/glove/) instead of Word2Vec. Furthermore, you can also try to run other graph theory algorithms, such as community detection algorithms, on the constructed social networks to uncover additional interesting insights. We hope that the methods and code presented in this notebook can assist you to solve other text analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"reading\"></a> 7. Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading material:\n",
    "- [Analysis of communities in a mythological social network, Miranda et al.](http://arxiv.org/abs/1306.2537)\n",
    "- [A survey of named entity recognition and classification, David Nadeau and Satoshi Sekine](http://nlp.cs.nyu.edu/sekine/papers/li07.pdf)\n",
    "- [Probabilistic topic models, David M. Blei](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf)\n",
    "- [LDAvis: A method for visualizing and interpreting topic models](http://stat-graphics.org/movies/ldavis.html)\n",
    "- [Practical deep text learning blog post](https://turi.com/learn/gallery/notebooks/deep_text_learning.html)\n",
    "- [Deep learning with word2vec and gensim](http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [gl-env]",
   "language": "python",
   "name": "Python [gl-env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
